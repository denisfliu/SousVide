import numpy as np
import os
import torch

import sousvide.synthesize.compress_helper as ch
import sousvide.control.network_helper as nh
import sousvide.visualize.rich_utilities as ru

from typing import Dict,Union,List,Tuple
from rich.progress import Progress
from sousvide.control.pilot import Pilot

def generate_observation_data(cohort:str,roster:List[str],
                              networks:Union[List[str],None]=None,
                              subsample:float=1.0) -> None:
    """
    Takes rollout data and generates observations for each pilot in the cohort. The observations are
    generated by running the rollout data through the pilot's OODA function. The observations are saved
    to a .pt file in the pilot's directory.

    Args:
        cohort:             Cohort name.
        roster:             List of student names.
        subsample:          Subsample ratio for the observations.

    """

    # Initialize the progress variables
    console = ru.get_console()
    progress = ru.get_generation_progress()
    subunits = "dpts"
    obsv_desc1 = "[bold dark_green]Generating observations...[/]"
    obsv_desc2 = "[bold dark_green]Saving dataset...[/]"
    
    # Generate some useful intermediate variables
    workspace_path = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    cohort_path = os.path.join(workspace_path,"cohorts",cohort)
    rollout_folder_path = os.path.join(cohort_path,"rollout_data")

    # Extract some useful diagnostics info
    courses = [folder for folder in os.listdir(rollout_folder_path)]

    Nds = 0
    for course in courses:
        dset_files = os.listdir(os.path.join(rollout_folder_path, course, "trajectories"))
        Nds += len(dset_files)

    # Preface Message
    console.print(f"Generating observation data with subsample ratio {subsample} for...\n",
                  f"Cohort : [green4]{cohort}[/]\n",
                  f"Roster : {roster}\n",
                  f"Courses: {courses}")

    with progress:
        # Initialize observations progress bar
        obsv_task = progress.add_task(obsv_desc1,total=None,units='samples')

        # Extract observations for each student
        for student in roster:
            # Load the Pilot
            pilot = Pilot(cohort,student)

            # Check if the student has desired networks
            if networks is not None:
                if set(networks).isdisjoint(pilot.policy.networks):
                    console.print(f"Pilot [bold cyan]{student}[/] does not have network [bold cyan]{networks}[/].")
                    continue

            # Initialize student progress bar
            Ndata = 0
            network_names = "\["+",".join(list(pilot.policy.networks.keys()))+"]"
            student_tname = f"[bold green3]{student:>8} > {network_names}[/]\n     [dark_green]Data Count:[/]"
            student_desc = ru.get_data_description(student_tname,Ndata,subunits=subunits,Nmn=20)
            student_task = progress.add_task(student_desc,total=Nds, units='datasets')
            
            for course in courses:
                # Load Trajectory Data
                ro_data_path = os.path.join(rollout_folder_path,course)
                traj_data_folder = os.path.join(ro_data_path,"trajectories")
                imgs_data_folder = os.path.join(ro_data_path,"images")
                traj_data_files = sorted([
                    file for file in os.listdir(traj_data_folder)])
                imgs_data_files = sorted([
                    file for file in os.listdir(imgs_data_folder)])
                
                # Generate Observation Data
                for idx_ds,(traj_data_file,imgs_data_file) in enumerate(zip(traj_data_files,imgs_data_files)):
                    # Load the Data
                    traj_dataset = torch.load(os.path.join(traj_data_folder,traj_data_file))
                    imgs_dataset = torch.load(os.path.join(imgs_data_folder,imgs_data_file))

                    # Reset the observations progress bar
                    progress.reset(obsv_task,description=obsv_desc1,total=len(traj_dataset))
                    obsv_bar = (progress,obsv_task)
    
                    # Extract the Observations
                    observations,Nobs = generate_observations(pilot,
                                                              traj_dataset,imgs_dataset,
                                                              subsample,obsv_bar)
                    
                    # Update the observations progress bar
                    progress.update(obsv_task,description=obsv_desc2)

                    # Save the observations
                    save_observations(cohort_path,course,pilot.name,observations,idx_ds,networks)

                    # Update the number of data points
                    Ndata += Nobs

                    # Update the progress bar
                    student_desc = ru.get_data_description(student_tname,Ndata,subunits=subunits)
                    progress.update(student_task,description=student_desc,advance=1)

                # Cap off progress update
                progress.refresh()

def generate_observations(pilot:Pilot,
                          traj_set:Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],
                          imgs_set:Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],
                          subsample:float=1,
                          progress_bar:Tuple[Progress,int]=None) -> Tuple[Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],int]:
    
    # Initialize the observation data dictionary
    Observations = []

    # Unpack augmenteation variables
    aug_type = np.array(pilot.da_cfg["type"])
    aug_mean = np.array(pilot.da_cfg["mean"])
    aug_std = np.array(pilot.da_cfg["std"])

    # Set subsample step
    nss = int(1/subsample)

    Nobs = 0
    for traj_data,imgs_data in zip(traj_set,imgs_set):
        # Unpack data
        Tro,Xro = traj_data["Tro"],traj_data["Xro"]
        Uro,Fro = traj_data["Uro"],traj_data["Fro"]
        obj,Ndata = traj_data["obj"],traj_data["Ndata"]
        rollout_id = traj_data["rollout_id"]
        frame = traj_data["frame"]
        params = [frame["mass"],frame["force_normalized"]]
    
        # Decompress and extract the image data if compressed
        Imgs = ch.decompress_data(imgs_data)["images"]

        # Check if images are raw or processed. Raw images are in (B,H,W,C) format while
        # processed images are in (B,C,H,W) format.
        height,width = Imgs.shape[1],Imgs.shape[2]

        if height == 224 and width == 224:
            Imgs = np.transpose(Imgs, (0, 3, 1, 2))

        Xnn,Ynn = [],[]
        unn_pr = np.zeros(4)
        znn_cr = {key: torch.zeros(pilot.policy.Nznn[key]) for key in pilot.policy.Nznn.keys()}
        for k in range(Ndata):
            # Generate current state (with/without noise augmentation)
            if aug_type == "additive":
                x_cr = Xro[:,k] + np.random.normal(aug_mean,aug_std)
            elif aug_type == "multiplicative":
                x_cr = Xro[:,k] * (1 + np.random.normal(aug_mean,aug_std))
            else:
                x_cr = Xro[:,k]

            # Extract other data
            t_cr = Tro[k]
            tx_cr = np.hstack((Tro[k],Xro[:,k]))
            unn_cr,f_cr = Uro[:,k],Fro[:,k]
            img_cr = Imgs[k,:,:,:]

            # Compute the source labels
            ynn_srcs = {
                "current": torch.tensor(tx_cr,dtype=torch.float32).unsqueeze(0),
                "parameters": torch.tensor(params,dtype=torch.float32).unsqueeze(0),
                "forces": torch.tensor(f_cr,dtype=torch.float32).unsqueeze(0),
                "command": torch.tensor(unn_cr,dtype=torch.float32).unsqueeze(0),
            }

            # Rollout and collect the inputs
            _,znn_cr,xnn_cr,_ = pilot.OODA(unn_pr,t_cr,x_cr,obj,img_cr,znn_cr)

            # Extract the labels from source and trim inputs if they don't exist
            ynn_cr = {}
            for xnn_key in xnn_cr.keys():
                ynn_idxs = pilot.policy.networks[xnn_key].label_indices

                try:
                    ynn_cr[xnn_key] = nh.extract_io(ynn_srcs,ynn_idxs,use_tensor=True)
                except:
                    ynn_cr[xnn_key] = None

            # Collect data conditioned on subsample step and history window
            if k % nss == 0 and k >= pilot.policy.nhy:
                Xnn.append(xnn_cr)
                Ynn.append(ynn_cr)

            # Loop updates
            unn_pr = unn_cr

        # Update the progress bar
        if progress_bar is not None:
            progress,obsv_task = progress_bar
            progress.update(obsv_task,advance=1)

        # Store the observation data
        observations = {
            "Xnn":Xnn,
            "Ynn":Ynn,
            "Ndata":len(Xnn),
            "rollout_id":rollout_id,
            "frame":frame
        }
        Observations.append(observations)

        # Update the counters
        Nobs += len(Xnn)

    return Observations,Nobs

def save_observations(cohort_name:str,course_name:str,
                      pilot_name:str,
                      Observations:List[Dict[str,List[Dict[str,torch.Tensor]]]],
                      idx_set:int,networks:Union[List[str],None]=None) -> None:
    
    """
    Saves the observation data to a .pt file in folders corresponding to pilot name within the course
    directory within the cohort directory.

    Args:
        cohort_path:    Cohort path.
        course_name:    Name of the course.
        pilot_name:     Name of the pilot.
        Observations:   Observation data.
        idx_set:        Index of the observation data set.
        networks:       List of network names to filter observations (optional).

    Returns:
        None:           (observation data saved to cohort directory)
    """
    
    # Some useful path(s)
    workspace_path = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    cohort_path = os.path.join(workspace_path,"cohorts",cohort_name)

    # Use first observation to get relevant keys
    syllabus = []
    for key,value in Observations[0]["Ynn"][0].items():
        if value is not None:
            if networks is None or key in networks:
                syllabus.append(key)
    
    for topic_name in syllabus:
        # Create the topic directory if it doesn't exist
        topic_path = os.path.join(cohort_path,"observation_data",pilot_name,topic_name,course_name)
        if not os.path.exists(topic_path):
            os.makedirs(topic_path)

        # Save the observation data
        data_path = os.path.join(topic_path,"observations"+str(idx_set+1).zfill(3)+".pt")

        Xnn,Ynn = [],[]
        for observations in Observations:
            for xnn,ynn in zip(observations["Xnn"],observations["Ynn"]):
                Xnn.append(xnn[topic_name])
                Ynn.append(ynn[topic_name])
        data = {"Xnn":Xnn,"Ynn":Ynn,"Ndata":len(Ynn)}
        
        torch.save(data,data_path)