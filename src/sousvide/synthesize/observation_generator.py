import numpy as np
import os
import torch
import sousvide.synthesize.synthesize_helper as sh
import sousvide.control.network_helper as nh

from typing import Dict,Union,List,Tuple
from sousvide.control.pilot import Pilot
from rich.console import Console
from rich.progress import Progress
import time
def generate_observation_data(cohort:str,roster:List[str],subsample:float=1.0):
    """
    Takes rollout data and generates observations for each pilot in the cohort. The observations are
    generated by running the rollout data through the pilot's OODA function. The observations are saved
    to a .pt file in the pilot's directory.

    """

    # Initialize Console
    console = Console()

    # Generate some useful intermediate variables
    workspace_path = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    cohort_path = os.path.join(workspace_path,"cohorts",cohort)
    rollout_folder_path = os.path.join(cohort_path,"rollout_data")

    courses = [folder for folder in os.listdir(rollout_folder_path)]
    Nds = sum([1 for course in courses for file in os.listdir(os.path.join(rollout_folder_path, course)) if file.startswith("trajectories")])
    Ncs = len(courses)
    
    # Preface Message
    console.print(f"Generating observation data with subsample ratio {subsample} for...\n",
                  f"Cohort: {cohort}\n",
                  f"Roster: {roster}\n",
                  f"Courses: {courses}")

    with Progress() as progress:
        # Initialize PTasks
        roster_tasks = {}
        dataset_task = progress.add_task("Extracting Observations from File...")

        for student in roster:
            # Load the Pilot
            pilot = Pilot(cohort,student)
            networks = list(pilot.policy.networks.keys())
            
            # Add student to PTask
            roster_tasks[student] = progress.add_task(
                    f"[bold cyan]{student}[/bold cyan]: {networks}\n"
                    f"Ndata: 0 | Course: 0/{len(courses)} | File: 0/{Nds}",
                    total=Nds
            )

            Ndata = 0
            for idx_cs,course in enumerate(courses):
                # Load Trajectory Data
                rollout_data_path = os.path.join(rollout_folder_path,course)
                trajectory_data_files = sorted([
                    file for file in os.listdir(rollout_data_path)
                    if file.startswith("trajectories") and file.endswith(".pt")])
                image_data_files = sorted([
                    file for file in os.listdir(rollout_data_path)
                    if file.startswith("images") and file.endswith(".pt")])

                # Generate Observation Data
                Nds = len(trajectory_data_files)
                for idx_ds,(traj_data_file,imgs_data_file) in enumerate(zip(trajectory_data_files,image_data_files)):
                    # Load the Data
                    traj_dataset = torch.load(os.path.join(rollout_data_path,traj_data_file))
                    imgs_dataset = torch.load(os.path.join(rollout_data_path,imgs_data_file))

                    # Reset the Dataset PTask and pack it for observation generation tracking
                    progress.reset(dataset_task, total=len(traj_dataset))         # Reset for new course
                    progress_bar = (progress,dataset_task)

                    # Extract the Observations
                    observations,Nobs = generate_observations(pilot,
                                                              traj_dataset,imgs_dataset,
                                                              subsample,progress_bar)
                    Ndata += Nobs

                    # Save the observations
                    save_observations(cohort_path,course,pilot.name,observations,idx_ds)

                    # Update the PTasks
                    progress.update(
                        roster_tasks[student], 
                        advance=1, 
                        description=(
                            f"[bold cyan]{student}[/bold cyan]: {networks}\n"
                            f"Ndata: {Ndata} | Course: {idx_cs}/{Ncs} | File: {idx_ds+1}/{Nds}"
                        )
                    )

                # Update the roster progress
                progress.update(
                    roster_tasks[student], 
                    description=(
                        f"[bold cyan]{student}[/bold cyan]: {networks}\n"
                        f"Ndata: {Ndata} | Course: {idx_cs+1}/{Ncs} | File: {idx_ds+1}/{Nds}"
                    )
                )    

            # Cap off progress update
            progress.refresh()

def generate_observations(pilot:Pilot,
                          traj_set:Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],
                          imgs_set:Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],
                          subsample:float=1,progress_bar:Tuple[Progress,int]=None) -> Tuple[Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],int]:
    
    # Initialize the observation data dictionary
    Observations = []

    # Unpack the progress bar
    progress,task_id = progress_bar

    # Unpack augmenteation variables
    aug_type = np.array(pilot.da_cfg["type"])
    aug_mean = np.array(pilot.da_cfg["mean"])
    aug_std = np.array(pilot.da_cfg["std"])

    # Set subsample step
    nss = int(1/subsample)

    Nobs = 0
    for traj_data,imgs_data in zip(traj_set,imgs_set):
        # Unpack data
        Tro,Xro = traj_data["Tro"],traj_data["Xro"]
        Uro,Fro = traj_data["Uro"],traj_data["Fro"]
        obj,Ndata = traj_data["obj"],traj_data["Ndata"]
        rollout_id,course = traj_data["rollout_id"],traj_data["course"]
        frame = traj_data["frame"]
        params = [frame["mass"],frame["force_normalized"]]
    
        # Decompress and extract the image data
        Imgs = sh.decompress_data(imgs_data)["images"]

        # Check if images are raw or processed. Raw images are in (B,H,W,C) format while
        # processed images are in (B,C,H,W) format.
        height,width = Imgs.shape[1],Imgs.shape[2]

        if height == 224 and width == 224:
            Imgs = np.transpose(Imgs, (0, 3, 1, 2))

        Xnn,Ynn = [],[]
        unn_pr = np.zeros(4)
        znn_cr = {key: torch.zeros(pilot.policy.Nznn[key]) for key in pilot.policy.Nznn.keys()}
        for k in range(Ndata):
            # Generate current state (with/without noise augmentation)
            if aug_type == "additive":
                x_cr = Xro[:,k] + np.random.normal(aug_mean,aug_std)
            elif aug_type == "multiplicative":
                x_cr = Xro[:,k] * (1 + np.random.normal(aug_mean,aug_std))
            else:
                x_cr = Xro[:,k]

            # Extract other data
            t_cr = Tro[k]
            tx_cr = np.hstack((Tro[k],Xro[:,k]))
            unn_cr,f_cr = Uro[:,k],Fro[:,k]
            img_cr = Imgs[k,:,:,:]

            # Compute the source labels
            ynn_srcs = {
                "current": torch.tensor(tx_cr,dtype=torch.float32).unsqueeze(0),
                "parameters": torch.tensor(params,dtype=torch.float32).unsqueeze(0),
                "forces": torch.tensor(f_cr,dtype=torch.float32).unsqueeze(0),
                "command": torch.tensor(unn_cr,dtype=torch.float32).unsqueeze(0),
            }

            # Rollout and collect the inputs
            _,znn_cr,xnn_cr,_ = pilot.OODA(unn_pr,t_cr,x_cr,obj,img_cr,znn_cr)

            # Extract the labels from source and trim inputs if they don't exist
            ynn_cr = {}
            for xnn_key in xnn_cr.keys():
                ynn_idxs = pilot.policy.networks[xnn_key].label_indices

                try:
                    ynn_cr[xnn_key] = nh.extract_io(ynn_srcs,ynn_idxs,use_tensor=True)
                except:
                    ynn_cr[xnn_key] = None

            # Collect data conditioned on subsample step and history window
            if k % nss == 0 and k >= pilot.policy.nhy:
                Xnn.append(xnn_cr)
                Ynn.append(ynn_cr)

            # Loop updates
            unn_pr = unn_cr

        # Update the progress bar if it exists
        if progress_bar is not None:
            progress.update(task_id, advance=1)

        # Store the observation data
        observations = {
            "Xnn":Xnn,
            "Ynn":Ynn,
            "Ndata":len(Xnn),
            "rollout_id":rollout_id,
            "course":course,"frame":frame
        }
        Observations.append(observations)

        # Update the counters
        Nobs += len(Xnn)

    return Observations,Nobs

def save_observations(cohort_path:str,course_name:str,
                      pilot_name:str,
                      Observations:List[Dict[str,List[Dict[str,torch.Tensor]]]],
                      idx_set:int) -> None:
    
    """
    Saves the observation data to a .pt file in folders corresponding to pilot name within the course
    directory within the cohort directory.

    Args:
        cohort_path:    Cohort path.
        course_name:    Name of the course.
        pilot_name:     Name of the pilot.
        Observations:   Observation data.
        idx_set:        Index of the observation data set.

    Returns:
        None:           (observation data saved to cohort directory)
    """

    # Use first observation to get relevant keys
    syllabus = []
    for key,value in Observations[0]["Ynn"][0].items():
        if value is not None:
            syllabus.append(key)
    
    for topic_name in syllabus:
        # Create the topic directory if it doesn't exist
        topic_path = os.path.join(cohort_path,"observation_data",pilot_name,topic_name,course_name)
        if not os.path.exists(topic_path):
            os.makedirs(topic_path)

        # Save the observation data
        data_path = os.path.join(topic_path,"observations"+str(idx_set).zfill(3)+".pt")

        Xnn,Ynn = [],[]
        for observations in Observations:
            for xnn,ynn in zip(observations["Xnn"],observations["Ynn"]):
                Xnn.append(xnn[topic_name])
                Ynn.append(ynn[topic_name])
        data = {"Xnn":Xnn,"Ynn":Ynn,"Ndata":len(Ynn)}
        
        torch.save(data,data_path)