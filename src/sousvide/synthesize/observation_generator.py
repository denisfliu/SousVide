import numpy as np
import os
import torch

from typing import Dict,Union,List

import sousvide.synthesize.synthesize_helper as sh
import sousvide.control.network_helper as nh

from sousvide.control.pilot import Pilot

def generate_observation_data(cohort:str,roster:List[str],subsample:float=1.0):
    """
    Takes rollout data and generates observations for each pilot in the cohort. The observations are
    generated by running the rollout data through the pilot's OODA function. The observations are saved
    to a .pt file in the pilot's directory.

    """
    
    # Generate some useful paths
    workspace_path = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    cohort_path = os.path.join(workspace_path,"cohorts",cohort)
    rollout_folder_path = os.path.join(cohort_path,"rollout_data")

    # Initialize Pilots
    Pilots = [Pilot(cohort,name) for name in roster]
    
    print("==========================================================================================")

    for pilot in Pilots:
        # Print some useful information
        print(f"Pilot Name  : {pilot.name}")
        print(f"Augmentation: {pilot.da_cfg['mean']}")
        print( "            :                        +/-")
        print(f"            : {pilot.da_cfg['std']}")
        print(f"Subsample   : 1 in {int(1/subsample)}")
        print("------------------------------------------------------------------------------------------")

        # Get Course Folders
        courses = [folder for folder in os.listdir(rollout_folder_path)]
        
        Ndata = 0
        for course in courses:
            # Load Trajectory Data
            rollout_data_path = os.path.join(rollout_folder_path,course)
            trajectory_data_files = sorted([
                file for file in os.listdir(rollout_data_path)
                if file.startswith("trajectories") and file.endswith(".pt")])
            image_data_files = sorted([
                file for file in os.listdir(rollout_data_path)
                if file.startswith("images") and file.endswith(".pt")])
            
            # Generate Observation Data
            for idx,(traj_data_file,imgs_data_file) in enumerate(zip(trajectory_data_files,image_data_files)):
                # Load the Data
                traj_data_set = torch.load(os.path.join(rollout_data_path,traj_data_file))
                imgs_data_set = torch.load(os.path.join(rollout_data_path,imgs_data_file))
                
                # Extract the Observations
                observations,Nobs = generate_observations(pilot,traj_data_set,imgs_data_set,subsample)
                Ndata += Nobs

                # Save the observations
                save_observations(cohort_path,course,pilot.name,observations,idx)

        print("Data Counts ------------------------------------------------------------------------------")
        print("Extracted",Ndata,"observations from",len(courses),"course(s).")
        print("------------------------------------------------------------------------------------------")

    print("==========================================================================================")


def generate_observations(pilot:Pilot,
                          traj_set:Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],
                          imgs_set:Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]],
                          subsample:float=1) -> Dict[str,Union[str,int,Dict[str,Union[np.ndarray,float,str]]]]:
    
    # Initialize the observation data dictionary
    Observations = []

    # Unpack augmenteation variables
    aug_type = np.array(pilot.da_cfg["type"])
    aug_mean = np.array(pilot.da_cfg["mean"])
    aug_std = np.array(pilot.da_cfg["std"])

    # Set subsample step
    nss = int(1/subsample)

    # Generate observations
    Nobs = 0
    for traj_data,imgs_data in zip(traj_set,imgs_set):
        # Unpack data
        Tro,Xro = traj_data["Tro"],traj_data["Xro"]
        Uro,Fro = traj_data["Uro"],traj_data["Fro"]
        obj,Ndata = traj_data["obj"],traj_data["Ndata"]
        rollout_id,course = traj_data["rollout_id"],traj_data["course"]
        frame = traj_data["frame"]
        params = [frame["mass"],frame["force_normalized"]]
      
        # Decompress and extract the image data
        Imgs = sh.decompress_data(imgs_data)["images"]

        # Check if images are raw or processed. Raw images are in (B,H,W,C) format while
        # processed images are in (B,C,H,W) format.
        height,width = Imgs.shape[1],Imgs.shape[2]

        if height == 224 and width == 224:
            Imgs = np.transpose(Imgs, (0, 3, 1, 2))

        # Create Observation Data
        Xnn,Ynn = [],[]
        unn_pr = np.zeros(4)
        znn_cr = {key: torch.zeros(pilot.policy.Nznn[key]) for key in pilot.policy.Nznn.keys()}
        for k in range(Ndata):
            # Generate current state (with/without noise augmentation)
            if aug_type == "additive":
                x_cr = Xro[:,k] + np.random.normal(aug_mean,aug_std)
            elif aug_type == "multiplicative":
                x_cr = Xro[:,k] * (1 + np.random.normal(aug_mean,aug_std))
            else:
                x_cr = Xro[:,k]

            # Extract other data
            t_cr = Tro[k]
            tx_cr = np.hstack((Tro[k],Xro[:,k]))
            unn_cr,f_cr = Uro[:,k],Fro[:,k]
            img_cr = Imgs[k,:,:,:]

            # Compute the source labels
            ynn_srcs = {
                "current": torch.tensor(tx_cr,dtype=torch.float32).unsqueeze(0),
                "parameters": torch.tensor(params,dtype=torch.float32).unsqueeze(0),
                "forces": torch.tensor(f_cr,dtype=torch.float32).unsqueeze(0),
                "command": torch.tensor(unn_cr,dtype=torch.float32).unsqueeze(0),
            }

            # Rollout and collect the inputs
            _,znn_cr,xnn_cr,_ = pilot.OODA(unn_pr,t_cr,x_cr,obj,img_cr,znn_cr)

            # Extract the labels from source and trim inputs if they don't exist
            ynn_cr = {}
            for xnn_key in xnn_cr.keys():
                if xnn_key == "all":
                    ynn_idxs = pilot.policy.label_indices
                else:
                    ynn_idxs = pilot.policy.networks[xnn_key].label_indices

                try:
                    ynn_cr[xnn_key] = nh.extract_io(ynn_srcs,ynn_idxs,use_tensor=True)
                except:
                    ynn_cr[xnn_key] = None

            # Collect data conditioned on subsample step and history window
            if k % nss == 0 and k >= pilot.policy.nhy:
                Xnn.append(xnn_cr)
                Ynn.append(ynn_cr)

            # Loop updates
            unn_pr = unn_cr

        # Store the observation data
        observations = {
            "Xnn":Xnn,
            "Ynn":Ynn,
            "Ndata":len(Xnn),
            "rollout_id":rollout_id,
            "course":course,"frame":frame
        }
        Observations.append(observations)

        # Update the observation count
        Nobs += len(Xnn)

    return Observations,Nobs

def save_observations(cohort_path:str,course_name:str,
                      pilot_name:str,
                      Observations:List[Dict[str,List[Dict[str,torch.Tensor]]]],
                      idx_set:int) -> None:
    
    """
    Saves the observation data to a .pt file in folders corresponding to pilot name within the course
    directory within the cohort directory.

    Args:
        cohort_path:    Cohort path.
        course_name:    Name of the course.
        pilot_name:     Name of the pilot.
        Observations:   Observation data.
        idx_set:        Index of the observation data set.

    Returns:
        None:           (observation data saved to cohort directory)
    """

    # Use first observation to get relevant keys
    syllabus = []
    for key,value in Observations[0]["Ynn"][0].items():
        if value is not None:
            syllabus.append(key)
    
    for topic_name in syllabus:
        # Create the topic directory if it doesn't exist
        topic_path = os.path.join(cohort_path,"observation_data",pilot_name,topic_name,course_name)
        if not os.path.exists(topic_path):
            os.makedirs(topic_path)

        # Save the observation data
        data_path = os.path.join(topic_path,"observations"+str(idx_set).zfill(3)+".pt")

        Xnn,Ynn = [],[]
        for observations in Observations:
            for xnn,ynn in zip(observations["Xnn"],observations["Ynn"]):
                Xnn.append(xnn[topic_name])
                Ynn.append(ynn[topic_name])
        data = {"Xnn":Xnn,"Ynn":Ynn,"Ndata":len(Ynn)}
        
        torch.save(data,data_path)